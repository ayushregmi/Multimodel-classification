{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \n",
    "    '''\n",
    "    Load image and text data from the specified directory structure.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): Path to the root directory containing subdirectories for images and OCR text.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing image paths, text file paths, and corresponding class labels.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    image_data = []\n",
    "    text_data = []\n",
    "    labels = []\n",
    "\n",
    "    for class_dir in os.listdir(os.path.join(path, 'images')):\n",
    "        if os.path.isdir(os.path.join(path, 'images', class_dir)):\n",
    "            \n",
    "            # get the image and text directory paths\n",
    "            image_files = os.listdir(os.path.join(path, 'images', class_dir))\n",
    "            text_files = os.listdir(os.path.join(path, 'ocr', class_dir))\n",
    "            label = int(class_dir)\n",
    "            \n",
    "            # get the images and text paths\n",
    "            for image_file, text_file in zip(image_files, text_files):\n",
    "                image_path = os.path.join(path, 'images', class_dir, image_file)\n",
    "                text_path = os.path.join(path, 'ocr', class_dir, text_file)\n",
    "                \n",
    "                image_data.append(image_path)\n",
    "                text_data.append(text_path)\n",
    "                labels.append(label)\n",
    "\n",
    "    return pd.DataFrame(data=[image_data, text_data, labels]).T.rename({0:\"img_path\", 1:\"text_path\", 2:\"target\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>text_path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/images/0/466c6bc2-3196-499e-b506-8f0bda...</td>\n",
       "      <td>./data/ocr/0/412056de-bfed-4053-90b0-8fb3f6b9d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/images/0/88e2faad-f4c7-4331-beda-8a5f8a...</td>\n",
       "      <td>./data/ocr/0/7d92b943-8506-427e-ac1c-f8888d38b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/images/0/005a7f78-9e3f-44b3-beb8-3fe834...</td>\n",
       "      <td>./data/ocr/0/005a7f78-9e3f-44b3-beb8-3fe834af4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/images/0/00b8e9c9-76c8-431d-9584-16f37f...</td>\n",
       "      <td>./data/ocr/0/00b8e9c9-76c8-431d-9584-16f37f138...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/images/0/00d62096-c544-44d6-84a4-3fd3df...</td>\n",
       "      <td>./data/ocr/0/00d62096-c544-44d6-84a4-3fd3df471...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path  \\\n",
       "0  ./data/images/0/466c6bc2-3196-499e-b506-8f0bda...   \n",
       "1  ./data/images/0/88e2faad-f4c7-4331-beda-8a5f8a...   \n",
       "2  ./data/images/0/005a7f78-9e3f-44b3-beb8-3fe834...   \n",
       "3  ./data/images/0/00b8e9c9-76c8-431d-9584-16f37f...   \n",
       "4  ./data/images/0/00d62096-c544-44d6-84a4-3fd3df...   \n",
       "\n",
       "                                           text_path target  \n",
       "0  ./data/ocr/0/412056de-bfed-4053-90b0-8fb3f6b9d...      0  \n",
       "1  ./data/ocr/0/7d92b943-8506-427e-ac1c-f8888d38b...      0  \n",
       "2  ./data/ocr/0/005a7f78-9e3f-44b3-beb8-3fe834af4...      0  \n",
       "3  ./data/ocr/0/00b8e9c9-76c8-431d-9584-16f37f138...      0  \n",
       "4  ./data/ocr/0/00d62096-c544-44d6-84a4-3fd3df471...      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_data(\"./data\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDataset:\n",
    "    \n",
    "    '''\n",
    "    Dataset class for processing document images and text data.\n",
    "\n",
    "    This class loads document images and corresponding OCR text data, preprocesses them,\n",
    "    and prepares them for input into a model. It applies image transformations and tokenizes\n",
    "    text using the BERT tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): DataFrame containing image paths and text file paths.\n",
    "    - target (array-like): Array containing class labels.\n",
    "\n",
    "    Attributes:\n",
    "    - data (pd.DataFrame): DataFrame containing image paths and text file paths.\n",
    "    - target (array-like): Array containing class labels.\n",
    "    - tokenizer (BertTokenizer): BERT tokenizer for tokenizing text.\n",
    "    - image_transform (torchvision.transforms.Compose): Image transformation pipeline.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"img_path\"][idx]\n",
    "        text_path = self.data[\"text_path\"][idx]\n",
    "        \n",
    "        output = self.target[idx]\n",
    "        \n",
    "        # loading image\n",
    "        img_data = cv2.imread(img_path)\n",
    "        img_data = Image.fromarray(cv2.cvtColor(img_data, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        img_data = self.image_transform(img_data)\n",
    "        \n",
    "        # loading text\n",
    "        with open(text_path, 'r') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        tokens = self.tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "        \n",
    "        tokens.pop(\"token_type_ids\")\n",
    "        \n",
    "        return [img_data, tokens], torch.tensor(output)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    ResNet18 with modified final dense layer.\n",
    "\n",
    "    This class loads a pretrained ResNet18 model and modifies it by freezing\n",
    "    all layers except the final dense layer. The final dense layer is replaced\n",
    "    with a new linear layer with 256 output features.\n",
    "\n",
    "    Attributes:\n",
    "    - resnet (torchvision.models.ResNet): The pretrained ResNet18 model.\n",
    "    - fc (torch.nn.Linear): The modified final dense layer.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        \n",
    "        # loading pretrained resnet\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # freezeing all layers except the final dense layer\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # setting the final dense layer to trainable\n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        # modifying the final layer\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(in_features=num_features, out_features=256)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \n",
    "        '''\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "        - data (torch.Tensor): Input data with shape (batch, channels, width, height).\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Output tensor from the network.\n",
    "        '''\n",
    "        \n",
    "        # data -> (batch, channels, width, height)\n",
    "        \n",
    "        out = self.resnet(data)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    BERT model with a modified final dense layer.\n",
    "\n",
    "    This class loads a pretrained BERT model and adds a final dense layer for\n",
    "    downstream task fine-tuning. By default, it uses the 'bert-base-uncased'\n",
    "    pretrained model from Hugging Face's transformers library.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): The name of the pretrained BERT model to load. Default is 'bert-base-uncased'.\n",
    "\n",
    "    Attributes:\n",
    "    - model_name (str): The name of the pretrained BERT model being used.\n",
    "    - bert_model (transformers.BertModel): The pretrained BERT model.\n",
    "    - fc (torch.nn.Linear): The final dense layer added for fine-tuning.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super(Bert, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "        # freezing the layers\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # adding a final dense layer\n",
    "        self.fc = nn.Linear(in_features=self.bert_model.config.hidden_size, out_features=256)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        '''\n",
    "        Performs a forward pass through the BERT model.\n",
    "\n",
    "        Parameters:\n",
    "        - text (dict): Dictionary containing 'input_ids' and 'attention_mask' tensors representing tokenized input text.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Output tensor from the final dense layer.\n",
    "\n",
    "        '''\n",
    "        \n",
    "        # print(text['input_ids'].shape, text[\"attention_mask\"].shape)\n",
    "        \n",
    "        output = self.bert_model(text[\"input_ids\"].squeeze(1), text[\"attention_mask\"].squeeze(1))\n",
    "        \n",
    "        output = self.fc(output.pooler_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalClassification(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    A custom neural network model for multimodal classification tasks.\n",
    "\n",
    "    This model combines image and text modalities for classification using\n",
    "    separate pretrained models (ResNet18 for images and BERT for text). It\n",
    "    then concatenates the output features from both modalities and passes\n",
    "    them through a fully connected layer for classification.\n",
    "\n",
    "    Parameters:\n",
    "    - num_classes (int): The number of classes for classification. Default is 5.\n",
    "\n",
    "    Attributes:\n",
    "    - img_net (ResNet18): The ResNet18 model for processing image data.\n",
    "    - text_net (Bert): The BERT model for processing text data.\n",
    "    - fc (torch.nn.Sequential): The fully connected layer for final classification.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_classes=5):\n",
    "        \n",
    "        super(MultiModalClassification, self).__init__()\n",
    "        \n",
    "        self.img_net = ResNet18()\n",
    "        self.text_net = Bert()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=512, out_features=num_classes)\n",
    "            )\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        '''\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Parameters:\n",
    "        - data (list): List containing image data and text data.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Output tensor from the fully connected layer.\n",
    "        '''\n",
    "        \n",
    "        img_data = data[0]\n",
    "        text_data = data[1]\n",
    "        \n",
    "        img_outupt = self.img_net(img_data)\n",
    "        text_output = self.text_net(text_data)\n",
    "        \n",
    "        combined_output = torch.cat([img_outupt, text_output], dim=1)\n",
    "        \n",
    "        output = self.fc(combined_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def fit(self, dataloader, epochs=5, loss_func=nn.CrossEntropyLoss(), optimizer=optim.Adam, lr=0.01):\n",
    "        \n",
    "        '''\n",
    "        Trains the model using the provided dataloader.\n",
    "\n",
    "        Parameters:\n",
    "        - dataloader (torch.utils.data.DataLoader): Dataloader for training data.\n",
    "        - epochs (int): Number of epochs for training. Default is 5.\n",
    "        - loss_func: Loss function. Default is nn.CrossEntropyLoss().\n",
    "        - optimizer: Optimizer for training. Default is optim.Adam.\n",
    "        - lr (float): Learning rate for the optimizer. Default is 0.01.\n",
    "\n",
    "        '''\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        print(device)\n",
    "        \n",
    "        self.to(device=device)\n",
    "        \n",
    "        optimizer = optimizer([param for param in self.parameters() if param.requires_grad], lr=lr)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                \n",
    "                img_data = data[0].to(device=device)\n",
    "                text_data = data[1].to(device=device)\n",
    "                target = target.to(device=device)\n",
    "                \n",
    "                output = self([img_data, text_data])\n",
    "                \n",
    "                loss = loss_func(output, target)\n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(output, 1)\n",
    "                \n",
    "                total_samples += target.shape[0]\n",
    "                correct_predictions += (predicted == target).sum().item()\n",
    "                # print(f\"batch {batch_idx}\")\n",
    "                \n",
    "            print(f\"Epochs: {epoch+1}/{epochs}\\tLoss: {epoch_loss}\\taccuracy: {correct_predictions / total_samples}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "m = MultiModalClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[[\"img_path\", \"text_path\"]], data[\"target\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DocumentDataset({col: X_train[col].to_list() for col in X_train.columns}, y_train.to_list())\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epochs: 1/5\tLoss: 257.12941682338715\taccuracy: 0.4465\n",
      "Epochs: 2/5\tLoss: 32.21738764643669\taccuracy: 0.751\n",
      "Epochs: 3/5\tLoss: 20.08889175951481\taccuracy: 0.8005\n",
      "Epochs: 4/5\tLoss: 14.328647837042809\taccuracy: 0.849\n",
      "Epochs: 5/5\tLoss: 13.83181281387806\taccuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "m.fit(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(m, \"./model_multimodel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    \n",
    "    '''\n",
    "    Evaluates the performance of a model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The model to be evaluated.\n",
    "    - dataloader (DataLoader): DataLoader providing the evaluation dataset.\n",
    "\n",
    "    Returns:\n",
    "    - float: Accuracy of the model on the evaluation dataset.\n",
    "    '''\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            img_data = data[0].to(device=device)\n",
    "            text_data = data[1].to(device=device)\n",
    "            target = target.to(device=device)\n",
    "            \n",
    "            output = model([img_data, text_data])\n",
    "                    \n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            total_samples += target.shape[0]\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "            \n",
    "    return correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./model_multimodel.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.902"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = DocumentDataset({col: X_test[col].to_list() for col in X_test.columns}, y_test.to_list())\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "evaluate(model ,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
